---
title: 反向传播算法
date: 2018-11-26 10:31:34
tags: deeplearning
---
反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。<!--more-->该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。
反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的激励函数可微。
# 动机
任何监督式学习算法的目标是找到一个能把一组输入最好地映射到其正确的输出的函数。例如一个简单的分类任务，其中输入是动物的图像，正确的输出将是动物的名称。一些输入和输出模式可以很容易地通过单层神经网络（如感知器）学习。但是这些单层的感知机不能学习一些比较简单的模式，例如那些非线性可分的模式。例如，人可以通过识别动物的图像的某些特征进行分类，例如肢的数目，皮肤的纹理（无论是毛皮，羽毛，鳞片等），该动物的体型，以及种种其他特征。但是，单层神经网络必须仅仅使用图像中的像素的强度来学习一个输出一个标签函数。因为它被限制为仅具有一个层，所以没有办法从输入中学习到任何抽象特征。多层的网络克服了这一限制，因为它可以创建内部表示，并在每一层学习不同的特征。 第一层可能负责从图像的单个像素的输入学习线条的走向。第二层可能就会结合第一层所学并学习识别简单形状（如圆形）。每升高一层就学习越来越多的抽象特征，如上文提到的用来图像分类。每一层都是从它下方的层中找到模式，就是这种能力创建了独立于为多层网络提供能量的外界输入的内部表达形式。 反向传播算法的发展的目标和动机是找到一种训练的多层神经网络的方法，于是它可以学习合适的内部表达来让它学习任意的输入到输出的映射。
# 结构
反向传播算法（BP 算法）主要由两个阶段组成：激励传播与权重更新。
第1阶段：激励传播
每次迭代中的传播环节包含两步：
* （前向传播阶段）将训练输入送入网络以获得激励响应；
* （反向传播阶段）将激励响应同训练输入对应的目标输出求差，从而获得隐层和输出层的响应误差。
第2阶段：权重更新
对于每个突触上的权重，按照以下步骤进行更新：
* 将输入激励和响应误差相乘，从而获得权重的梯度；
* 将这个梯度乘上一个比例并取反后加到权重上。
这个比例（百分比）将会影响到训练过程的速度和效果，因此成为“训练因子”。梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。
第 1 和第 2 阶段可以反复循环迭代，直到网络对输入的响应达到满意的预定的目标范围为止。
# 算法
初始化网络权值（通常是小的随机值）
do
    forEach 训练样本 ex
    prediction = neural-net-output(network, ex)  // 正向传递
    actual = teacher-output(ex)
    计算输出单元的误差 (prediction - actual)
    计算 ${\Delta w_{h}}$对于所有隐藏层到输出层的权值      // 反向传递
    计算 ${\displaystyle \Delta w_{i}}$对于所有输入层到隐藏层的权值                                     // 继续反向传递
    更新网络权值 // 输入层不会被误差估计改变
until 所有样本正确分类或满足其他停止标准
return 该网络
# 梯度下降法
梯度下降法背后的直观感受可以用假设情境进行说明。一个被卡在山上的人正在试图下山（即试图找到极小值）。大雾使得能见度非常低。因此，下山的道路是看不见的，所以他必须利用局部信息来找到极小值。他可以使用梯度下降法，该方法涉及到察看在他当前位置山的陡峭程度，然后沿着负陡度（即下坡）最大的方向前进。如果他要找到山顶（即极大值）的话，他需要沿着正陡度（即上坡）最大的方向前进。使用此方法，他会最终找到下山的路。不过，要假设山的陡度不能通过简单地观察得到，而需要复杂的工具测量，而这个工具此人恰好有。需要相当长的一段时间用仪器测量山的陡峭度，因此如果他想在日落之前下山，就需要最小化仪器的使用率。问题就在于怎样选取他测量山的陡峭度的频率才不致偏离路线。
在这个类比中，此人代表反向传播算法，而下山路径表示能使误差最小化的权重集合。山的陡度表示误差曲面在该点的斜率。他要前行的方向对应于误差曲面在该点的梯度。用来测量陡峭度的工具是微分（误差曲面的斜率可以通过对平方误差函数在该点求导数计算出来）。他在两次测量之间前行的距离（与测量频率成正比）是算法的学习速率。参见限制一节中对此类型“爬山”算法的限制的讨论。
# 学习模式
有可供选择的三种学习模式(Mode)：在线，批量和随机。在在线和随机学习，每次传播后被立即做一个权重的更新。在批量学习模式，权重更新前有许多传播发生。在线学习被用于提供新的型态(pattern)的连续流的动态环境。随机学习和批量学习都使用静态型态(pattern)的一个训练集合。随机学习以一个随机的顺序经过数据集合，以减少陷入局部极小值的机会。随机学习也比批量学习更快，因为权重在每次传播后被立即更新。然而批量学习将产生一个更加稳定下降到一个局部最小值，因为每次更新都是基于所有型态被进行的。
# 限制
* 结果可能会收敛到极值。如果有只有一个极小值，梯度下降的“爬山”策略一定可以起作用。然而，往往是误差曲面有许多局部最小值和最大值。如果梯度下降的起始点恰好介于局部最大值和局部最小值之间，则沿着梯度下降最大的方向会到达局部最小值。
* 梯度下降法可以找到局部最小值，而不是全局最小值。
* 从反向传播学习获得的收敛很慢。
* 在反向传播学习的收敛性不能保证。
    * 然而，收敛到全局最小值据说使用自适应终止条件得到保证。
* 反向传播学习不需要输入向量的标准化（normalization）；然而，标准化可提高性能。
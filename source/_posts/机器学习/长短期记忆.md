---
title: 长短期记忆
date: 2018-11-26 10:25:00
tags: [深度学习]
---

长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间递归神经网络（RNN），论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。
<!--more-->
# 递归神经网络
递归神经网络包含了一种循环，这种循环可以长时间的保存信息。
![](http://img.wanghaojun.cn/img/RNN-rolled.png)
在上图中，一块循环网络A将一个输入 $x_{t}$ ，并输出一个$h_{t}$,一个循环允许信息从网络的一个步骤传递到另一个步骤。
这些循环让RNN看起来有些神秘，但是如果你多想一下，这与普通的循环神经网络并没有很大的不同，可以将循环神经网路看作是同一网络的多个副本，每一个副本都将信息传递给后继者，如果我们把循环展开
![](http://img.wanghaojun.cn/img/RNN-unrolled.png)
这种类似链的性质表明，递归神经网络与序列和列表密切相关。它们是用于此类数据的神经网络和自然架构。
这种网络结构被证实非常有用，在过去的数年中，将RNN应用于各种问题取得了令人难以置信的成功：语音识别，语音建模，翻译，图像字幕等等。
这些成功的关键在于LSTM，这是一种非常特殊的RNN结构，对于许多任务而言，它比标准版本要好用的多，几乎所有的RNN神经网络都是用它们实现的。
# 长期依赖的问题
RNN的吸引力在于它能够将以前的信息连接到当前的任务，比如说使用先前的视频帧来理解现在的视频帧。如果RNN可以做到这一点，它们将非常有用。
有时，我们为了执行当前的任务需要查看最近的信息。举个例子，一个语言模型尝试通过前一个单词来预测下一个单词，如果我们尝试预测“the clouds are in sky”最后一个单词，我们不需要任何其他的内容，很明显下一个单词是sky。在这个例子中，相关信息和地点之间的间隙很小，RNN可以学习使用过去的信息。
但是，在一些情况下，我们需要更多的内容。尝试预测句子“I grew up in France...I speak fluent French”。最近的信息表明下一个单词可能是一个语言的名字，但是，如果我们继续缩小范围到哪一种语言，我们需要从前面获取“France”这个信息。相关信息和需要的点之间的间隙可能是非常大的。
不幸的是，随着间隙变大，RNN也不能学习去连接这些信息。
理论上，RNN绝对可以解决这种“长期依赖”的问题，一个人可以通过认真挑选参数的方式来解决这类问题。然而，在实践中，RNN似乎无法学习它们。Hochreiter（德国）和Bengio等人对该问题进行了深入探讨，它们找到了一些为什么很难学习的根本原因。
幸运的是，LSTM并没有这样的问题。
# LSTM网络
长端期记忆网络，通常被叫做“LSTM”，是RNN的一种特殊形式，能够像学习长期依赖，它们由Hochreiter和Schmidhuber介绍，并在以后的工作中被许多人提炼和推广。它们在各种各样的问题上表现的非常好，现在被广泛使用。
LSTM旨在避免长期依赖性的问题，长时间记住信息是它们的默认行为，而不是它们难以学习的东西。
所有的递归神经网络都有神经网络重复模块链的形式。在标准的RNN中，重复模块有非常简单的结构，比如说一个的单一的tanh（激活函数）层。
![](http://img.wanghaojun.cn/img/LSTM3-SimpleRNN.png)
LSTM也有这种链式结构，但是重复模块是一个不同的结构。LSTM有四个，而不是一个神经网络层，通过一种特殊的方式进行交互。
![](http://img.wanghaojun.cn/img/LSTM3-chain.png)
在上图中，每一行都携带一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。
# LSTM的核心思想
LSTM的关键是cell state（细胞状态），一条水平线贯穿整个图的顶部。
细胞状态类似于传送带，它之间沿整个链运行，只有一些微小的线性相互作用，信息很容易沿着它不变地流动。
![](http://img.wanghaojun.cn/img/LSTM3-SimpleRNN.png)
LSTM拥有添加或删除信息到细胞状态的能力，由一种叫做门的结构进行调节。门是一种可选择通过信息的方式。它们由sigmoid 神经网络层和逐点乘法运算组成。sigmoid层输出0到1之间的数字，描述每个组件应该通过多少。值为零意味着“不让任何东西通过”，而值为1则意味着“让一切都通过”。
LSTM具有三个这样的门，用于保护和控制细胞状态。
# LSTM的具体过程
LSTM的第一步决定我们将从细胞状态中抛弃哪些信息，这个决定由一个叫做“forget gate layer（遗忘门层）”的sigmoid层完成。它会将查看$h_{t-1}$和$x_t$，然后对于细胞状态$C_{t-1}$中的每个值输出一个0与1之间的数，1代表“完全保留这个”，而0代表“完全摆脱这个”。
我们回到语言模型的例子，尝试在先前的单词的基础上预测下一个单词。在这个问题中，细胞状态可能包含当前主题的性别，所以就可以使用正确的代词。当我们看到一个新的主题时，就会忘记旧主题的性别。
![](http://img.wanghaojun.cn/img/LSTM3-focus-f.png)
下一步是确定我们将在单元状态中存储哪些新的信息。这包含两部分，首先，一个叫“输入门”的sigmoid层决定我们要更新哪些值。接下来，一个tanh层创造新的候选值的向量，${\vec{C_t}}$，可能会添加到状态。在下一步中，我们将结合这两个来进行状态更新。
在我们的语言模型的例子中，我们想要将新主题的性别添加到细胞状态，以替换我们忘记的旧主题。
![](http://img.wanghaojun.cn/img/LSTM3-focus-i.png)
最后，我们需要决定我们要输出的内容。此输出将基于我们的单元状态，但将是过滤版本。首先，我们通过一个sigmoid层来决定细胞状态的哪一部分将被输出。然后，我们将单元状态通过tanh（将值推到-1和1之间）并将其乘以sigmoid门的输出，这样我们只输出我们决定的部分。
对于语言模型示例，由于它只是看到一个主题，它可能想要输出与动词相关的信息，以防接下来会发生什么。例如，它可能输出主语是单数还是复数，以便我们知道动词应该用什么形式。
![](http://img.wanghaojun.cn/img/LSTM3-focus-o.png)

本文转自
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
---
title: word2Vec
date: 2019-01-02 18:17:04
tags: [NLP]
---

word2Vec
word2vec的核心是神经网络，采用 CBOW（Continuous Bag-Of-Words，即连续的词袋模型）和 Skip-Gram 两种模型，将词语映像到同一坐标系，输出为数值型向量的方法。<!--more-->简而言之，就是将人类才可以看懂的文字，转换为机器也可以识别、操作、处理的数值，将一串文字转化为一个数值型向量的过程。这个词向量的维度一般情况下要远远小于词语总数 V 的大小，所以 Word2vec 本质上是一种降维操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。
由word2vec思想，即考察前后文的关联性产生了这两个模型：Skip-gram是通过一个词a去预测它周围的上下文；而CBOW相反，是通过上下文来预测其间的词。CBOW一般用于数据，而Skip-Gram通常用在数据量较大的情况。
CBOW：

![](http://img.wanghaojun.cn/img/20181226103700.jpg)

Skip-Gram

![](http://img.wanghaojun.cn/img/20181226101922.png)

CBOW给出了文本中一个词语wt左右各两个词，输出为wt。然后通过单隐藏层的神经网络去输出文本中每个词按照这个输入数据，输出为wt的概率。Skip-gram相反，用一个输入词wt去预测左右两个词语，同样输出概率值。
word2Vec的输入是one hot encode的向量。

![](http://img.wanghaojun.cn/img/20181222183129.png)

我们关注的点在隐藏层，如果我们想用将每个词表示为一个维度为100的向量，即每个词有100个特征，那么隐藏层就是一个10000行×100列的矩阵（也就是隐藏层有100个结点），这个10000×100的矩阵就是我们最后想要的结果，它会将10000个词中的每一个表示为一个100维的向量，有木有发现，10000维的词瞬间被降维到了100维，当然这个权重矩阵中的值就不是0-1了。
最终得到这个1×100的向量。在输出层通过softmax进行变换，使输出层每个结点将会输出一个0-1之间的值（概率），这些所有输出层结点的概率之和为1。
---
title: 激活函数
date: 2019-04-05 11:32:02
tags: [深度学习]
---
如果不在神经元中引入非线性激活函数的话，那么无论隐藏层有多少层，<!--more-->神经网络都在做线性函数运算，只是将输入进行线性组合而已，这种线性函数的隐藏层是没有用的。
在某些回归问题中，可能会在输出层使用线性激活函数，$g(z) = z$，比如说房价预测问题，这时候的输出应该是一个实数，取值范围是0到无穷大。
### sigmoid
sigmoid函数：$s=\sigma(w^T x+b)=\sigma(z)=\frac{1}{1+e^{-z}}$
![](http://img.wanghaojun.cn/img/20190401210938.png)
sigmoid的应用场景很少，因为一帮情况下tanh的性能都要优于sigmoid，该激活函数主要应用于二分法的输出层。
导数：$g'(z) = g(z)*(1-g(z))$
### tanh
$$g(z) =tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$$
![](http://img.wanghaojun.cn/img/20190405095659.png)
其值域为[-1,1]，平均值更接近于0，而不是sigmoid激活函数的0.5，所以有利于输入下一层的神经元进行进一步运算，但是由于可能为负值，所以不适合用于输出层，相比之下，tanh适合于隐藏层，sigmoid适合于输出层。
导数： $g'(z) = 1- (g(z))^2$
### Rectified Linear Unit(ReLU)
$$ ReLU:   g(z) = max(0,z) $$
![](http://img.wanghaojun.cn/img/20190405100836.png)
优点：当x很大时，导数不会像sigmoid或者tanh一样变小，所以进行梯度下降的时候，速度比前两者快的多。目前比较常用。
导数：
$g'(z) = 1 \quad  if \quad z>0$
$g'(z) = 0  \quad if \quad z <0$
当z=0，ReLU不存在导数，但是可以定义为1，或者0，对实际运算影响不大。下面leaky ReLU同样如此。
### leaky ReLU
$$ leaky \quad ReLU:   g(z) = max(0.01z,z) $$
![](http://img.wanghaojun.cn/img/20190405104546.png)
类似于ReLU，只不过当z<0时，函数值不再为0，导数也不为0.系数0.01是看实际情况具体选取的。
导数：
$g'(z) = 1 \quad  if \quad z>0$
$g'(z) = 0.01  \quad if \quad z <0$